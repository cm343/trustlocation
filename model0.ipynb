{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3737bad3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9f417dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b2b66",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2daec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388994b7",
   "metadata": {},
   "source": [
    "# Defs and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bf0b828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature specification for the model\n",
    "# author_name: string\n",
    "# number_authors_comments: integer , if unknown then -1\n",
    "# rating: integer (1-5), if unknonw then -1\n",
    "# text: string \n",
    "# general_location_type: string, one of general_location_type_categories\n",
    "# TODO specific_location_type: !!!UNUSED!!! string, if unknown then \"Unknown\"\n",
    "# time: timestamp, if unknown then -1\n",
    "# photo_attached: string, one of photo_attached_categories\n",
    "# TODO responses:  !!!UNUSED!!! string\n",
    "# number_of_responses: integer, if unknown then -1\n",
    "model_features = ['author_name' ,'number_authors_comments', 'rating', 'text','general_location_type', 'specific_location_type', 'time', 'photo_attached', 'responses','number_of_responses']\n",
    "general_location_type_categories= [\n",
    "    \"Automotive\",\n",
    "    \"Business\",\n",
    "    \"Culture\",\n",
    "    \"Education\",\n",
    "    \"Entertainment and Recreation\",\n",
    "    \"Facilities\",\n",
    "    \"Finance\",\n",
    "    \"Food and Drink\",\n",
    "    \"Geographical Areas\",\n",
    "    \"Government\",\n",
    "    \"Health and Wellness\",\n",
    "    \"Housing\",\n",
    "    \"Lodging\",\n",
    "    \"Natural Features\",\n",
    "    \"Places of Worship\",\n",
    "    \"Services\",\n",
    "    \"Shopping\",\n",
    "    \"Sports\",\n",
    "    \"Transportation\",\n",
    "    \"Unknown\"\n",
    "]\n",
    "photo_attached_categories = [\n",
    "    \"No\",\n",
    "    \"Yes\", \n",
    "    \"Unknown\"\n",
    "]\n",
    "\n",
    "def normalizer_model_features(df_in):\n",
    "    if(set(model_features) != set(df_in.columns)):\n",
    "        raise ValueError(\"Input dataframe does not have the correct columns\")\n",
    "    df_out = df_in.copy()\n",
    "    scaler = StandardScaler()\n",
    "    df_out[\"number_authors_comments\"] = scaler.fit_transform(df_in[[\"number_authors_comments\"]])\n",
    "    df_out[\"rating\"] = scaler.fit_transform(df_in[[\"rating\"]])\n",
    "    df_out[\"time\"] = scaler.fit_transform(df_in[[\"time\"]])\n",
    "    df_out[\"number_of_responses\"] = scaler.fit_transform(df_in[[\"number_of_responses\"]])\n",
    "    return df_out\n",
    "\n",
    "def encoder_model_features(df_in):\n",
    "    if(set(model_features) != set(df_in.columns)):\n",
    "        raise ValueError(\"Input dataframe does not have the correct columns\")\n",
    "    dummies = pd.get_dummies(df_in[['general_location_type', 'specific_location_type', 'photo_attached']], dtype=int)\n",
    "    df_in.drop(columns=['general_location_type', 'specific_location_type', 'photo_attached'], inplace=True)\n",
    "    df_out = pd.concat([df_in, dummies], axis=1)\n",
    "    return df_out\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# function to preprocess the dataframe to the input format required by the model\n",
    "# takes a dataframe with columns as in input_format\n",
    "def preprocess_features_scraped_data(df_in):\n",
    "    input_format = ['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp', 'gmap_id','label']\n",
    "\n",
    "    if not (set(input_format).issubset(set(df_in.columns))):\n",
    "        raise ValueError(\"Input dataframe does not have the correct columns\")\n",
    "    df_out = pd.DataFrame(columns=model_features) \n",
    "    df_out['author_name'] = df_in['name'].fillna(\"\").astype(str)\n",
    "    df_out['number_authors_comments'] = -1\n",
    "    df_out['rating'] = df_in['rating'].fillna(-1).astype(int)\n",
    "    df_out['text'] = df_in['text'].fillna(\"\").astype(str)\n",
    "    df_out['general_location_type'] = \"Unknown\"\n",
    "    df_out['specific_location_type'] = \"Unknown\"\n",
    "    df_out['time'] = df_in['time'].fillna(-1).astype(int)\n",
    "    df_out['photo_attached'] = df_in['pics'].apply(lambda x: \"No\" if pd.isna(x) else \"Yes\").astype(str)\n",
    "    df_out['number_of_responses'] = df_in['resp'].apply(lambda x: 0 if pd.isna(x) else 1 ) # TODO: currently only one rsponse if multiple, correct in future \n",
    "    df_out['responses'] = df_in['resp'].fillna(\"\").astype(str)\n",
    "    return df_out\n",
    "\n",
    "# function to preprocess the dataframe to the label format required by the model\n",
    "# 0 if still_exists is 'exists' else 1\n",
    "def preprocess_labels_scraped_data(df_in):\n",
    "    df_out = pd.DataFrame(columns=['label']) \n",
    "    df_out['label'] = df_in['still_exists'].apply(lambda x: 0 if x=='exists' else 1).astype(int)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def preprocess_features_1100(df_in):\n",
    "    # Expected input columns\n",
    "    input_format = ['business_name', 'author_name', 'text', 'photo', 'rating', 'rating_category', 'label']\n",
    "\n",
    "    if not set(input_format).issubset(df_in.columns):\n",
    "        raise ValueError(\"Input dataframe does not have the correct columns\")\n",
    "\n",
    "    # Create output DataFrame with the exact column order\n",
    "    df_out = pd.DataFrame(columns=model_features)\n",
    "\n",
    "    # Fill columns\n",
    "    df_out['author_name'] = df_in['author_name'].fillna(\"\").astype(str)\n",
    "    df_out['number_authors_comments'] = -1  # placeholder\n",
    "    df_out['rating'] = df_in['rating'].fillna(-1).astype(int)\n",
    "    df_out['text'] = df_in['text'].fillna(\"\").astype(str)\n",
    "    df_out['general_location_type'] = \"Unknown\"\n",
    "    df_out['specific_location_type'] = \"Unknown\"\n",
    "    df_out['time'] = -1  # placeholder if not in input\n",
    "    df_out['photo_attached'] = \"Yes\"  # always present\n",
    "    df_out['responses'] = \"\"  # placeholder\n",
    "    df_out['number_of_responses'] = 0  # placeholder\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# Preprocess labels using the mapping dictionary\n",
    "def preprocess_labels_1100(df_in):\n",
    "    convert_dir = {'GOOD': 0, 'ADVERTISEMENT': 1, 'IRRELEVANT': 2, 'RANTS': 3}\n",
    "    df_out = pd.DataFrame(columns=['label'])\n",
    "    df_out['label'] = df_in['label'].map(convert_dir).astype(int)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# feature extraction model setup\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "# function to extract features\n",
    "def feature_extraction(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    vector_cls = outputs.last_hidden_state[:, 0, :]  # shape: (1, 768)\n",
    "    return vector_cls\n",
    "def feature_extraction_batch(texts):\n",
    "    with torch.no_grad():\n",
    "        return torch.cat([feature_extraction(t) for t in texts])\n",
    "    \n",
    "\n",
    "\n",
    "# loaders\n",
    "def create_tensor_dataset(features, labels):\n",
    "    tabular_features_tensor = torch.tensor(features, dtype=torch.float32,requires_grad=False)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.int64,requires_grad=False)\n",
    "    return tabular_features_tensor, labels_tensor\n",
    "\n",
    "\n",
    "def reverse_weighted_dataloader(features, labels, batch_size):\n",
    "    class_counts_train = np.bincount(labels) # Count occurrences of integer to max integer\n",
    "    weights_train = 1 / class_counts_train # Inverse frequency\n",
    "    sample_weights_train = [weights_train[label] for label in labels] # Assign weight to each sample\n",
    "\n",
    "    sampler = WeightedRandomSampler(sample_weights_train, num_samples=len(sample_weights_train), replacement=True)\n",
    "    dataset_train = TensorDataset(features , labels)\n",
    "    loader_train = DataLoader(dataset_train, batch_size=batch_size, sampler=sampler)\n",
    "    return loader_train\n",
    "\n",
    "def shuffled_dataloader(features, labels, batch_size):\n",
    "    dataset_test = TensorDataset(features , labels)\n",
    "    loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "    return loader_test\n",
    "\n",
    "# model definition\n",
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim , hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(feature_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, feature_vec):\n",
    "        \"\"\"\n",
    "        text_vec: [batch_size, text_dim]   (e.g., DistilBERT CLS embeddings)\n",
    "        tabular_vec: [batch_size, tabular_dim]\n",
    "        \"\"\"\n",
    "        # Concatenate text and tabular features\n",
    "        x = feature_vec\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # logits\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train_one_epoch(loader_train, optimizer, model, loss_fn, epoch_index,report_interval,):# tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, data in enumerate(loader_train):\n",
    "        # Forward pass\n",
    "        feature_vec, labels = data\n",
    "        \n",
    "        # should not require grads\n",
    "        #print(\"text_vec.requires_grad:\", text_vec.requires_grad)\n",
    "        #print(\"tab_vec.requires_grad:\", tab_vec.requires_grad)\n",
    "        #print(\"labels.requires_grad:\", getattr(labels, \"requires_grad\", None))\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(feature_vec)\n",
    "        \n",
    "         # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "         # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % report_interval == report_interval - 1:\n",
    "            last_loss = running_loss / report_interval # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            #tb_x = epoch_index * len(loader_train) + i + 1\n",
    "            #tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f066ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.read_csv('data/1100_reviews_labeled_dummy_by_chatgpt.csv')\n",
    "\n",
    "\n",
    "# Convert labels to numbers\n",
    "#convert_dir = {'GOOD':0, 'ADVERTISEMENT': 1,  'IRRELEVANT':2, 'RANTS':3  }\n",
    "#df['num_label'] = [convert_dir[label] for label in df['label']]\n",
    "#print(df.head())\n",
    "\n",
    "\n",
    "# normalize tabular features\n",
    "#scaler = StandardScaler()\n",
    "#df[[\"rating\"]] = scaler.fit_transform(df[[\"rating\"]])\n",
    "#print(df.head()[\"rating\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa20d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      number_authors_comments    rating      time  number_of_responses  \\\n",
      "4973                      0.0 -3.512818  0.849650             1.013288   \n",
      "232                       0.0  0.324440 -0.453932             1.013288   \n",
      "4202                      0.0  0.324440  1.150455             1.013288   \n",
      "561                       0.0  0.324440  1.194277            -0.986886   \n",
      "3551                      0.0  0.324440  0.435221            -0.986886   \n",
      "\n",
      "      general_location_type_Unknown  specific_location_type_Unknown  \\\n",
      "4973                              1                               1   \n",
      "232                               1                               1   \n",
      "4202                              1                               1   \n",
      "561                               1                               1   \n",
      "3551                              1                               1   \n",
      "\n",
      "      photo_attached_No  photo_attached_Yes  \n",
      "4973                  1                   0  \n",
      "232                   1                   0  \n",
      "4202                  1                   0  \n",
      "561                   0                   1  \n",
      "3551                  1                   0  \n",
      "      label\n",
      "4973      0\n",
      "232       0\n",
      "4202      0\n",
      "561       0\n",
      "3551      0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example dataset\n",
    "num_samples = 1100\n",
    "text_dim = 768\n",
    "hidden_dim=128\n",
    "num_classes=4\n",
    "report_interval = 10\n",
    "num_epochs = 10\n",
    "batch_size=4\n",
    "\n",
    "df = pd.read_csv('review_other_head_labeled.csv')\n",
    "\n",
    "\n",
    "\n",
    "#features = preprocess_features_1100(df)\n",
    "#labels = preprocess_labels_1100(df)\n",
    "\n",
    "features = preprocess_features_scraped_data(df)\n",
    "labels = preprocess_labels_1100(df)\n",
    "features = normalizer_model_features(features)\n",
    "features = encoder_model_features(features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split into train (80%) and test (20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels  # stratify keeps class balance\n",
    ")\n",
    "\n",
    "drop = ['author_name' , 'text', 'responses',]\n",
    "x_train_tab = x_train.drop(columns=drop )\n",
    "x_test_tab = x_test.drop(columns=drop)\n",
    "\n",
    "print(x_train_tab.head())\n",
    "print(y_train.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d2aba2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embeddings = feature_extraction_batch(x_train['text'])\n",
    "x_test_embeddings = feature_extraction_batch(x_test['text']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fffca583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes:\n",
      "- feature_tensor_train : torch.Size([4000, 776])\n",
      "- x_train_embeddings   : torch.Size([4000, 768])\n",
      "- x_train_tab          : (4000, 8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_tensor_train_tab, label_tensor_test = create_tensor_dataset(x_train_tab.values, y_train['label'].values)\n",
    "feature_tensor_train = torch.cat((x_train_embeddings, feature_tensor_train_tab), dim=1)\n",
    "loader_train = reverse_weighted_dataloader(feature_tensor_train, label_tensor_test, batch_size)\n",
    "\n",
    "feature_tensor_test_tab, label_tensor_test = create_tensor_dataset(x_test_tab.values, y_test['label'].values)\n",
    "feature_tensor_test = torch.cat((x_test_embeddings, feature_tensor_test_tab), dim=1)\n",
    "loader_test = shuffled_dataloader(feature_tensor_test, label_tensor_test, batch_size)\n",
    "\n",
    "\n",
    "# Example setup\n",
    "model = HybridClassifier(feature_dim=feature_tensor_train.shape[1],hidden_dim=hidden_dim , num_classes=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"\"\"\n",
    "Shapes:\n",
    "- feature_tensor_train : {feature_tensor_train.shape}\n",
    "- x_train_embeddings   : {x_train_embeddings.shape}\n",
    "- x_train_tab          : {x_train_tab.shape}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8c93a61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 1.4986106276512146\n",
      "  batch 20 loss: 1.3550705194473267\n",
      "  batch 30 loss: 1.2688472986221313\n",
      "  batch 40 loss: 1.3258498072624207\n",
      "  batch 50 loss: 1.2192080080509187\n",
      "  batch 60 loss: 1.39317786693573\n",
      "  batch 70 loss: 1.1766167998313903\n",
      "  batch 80 loss: 1.2333898007869721\n",
      "  batch 90 loss: 1.0189742386341094\n",
      "  batch 100 loss: 1.178773248195648\n",
      "  batch 110 loss: 1.0297385454177856\n",
      "  batch 120 loss: 1.2078661799430848\n",
      "  batch 130 loss: 1.003592187166214\n",
      "  batch 140 loss: 0.9647981286048889\n",
      "  batch 150 loss: 1.0407690465450288\n",
      "  batch 160 loss: 0.9078296780586242\n",
      "  batch 170 loss: 0.8416253954172135\n",
      "  batch 180 loss: 0.9307743191719056\n",
      "  batch 190 loss: 0.9137235343456268\n",
      "  batch 200 loss: 0.9119169443845749\n",
      "  batch 210 loss: 0.864942866563797\n",
      "  batch 220 loss: 0.7623353630304337\n",
      "  batch 230 loss: 0.7790306866168976\n",
      "  batch 240 loss: 0.9792093276977539\n",
      "  batch 250 loss: 0.8960968926548958\n",
      "  batch 260 loss: 0.7324938952922821\n",
      "  batch 270 loss: 0.6575170755386353\n",
      "  batch 280 loss: 0.6452091097831726\n",
      "  batch 290 loss: 0.7028864741325378\n",
      "  batch 300 loss: 0.6127969294786453\n",
      "  batch 310 loss: 0.5448431849479676\n",
      "  batch 320 loss: 0.7611991137266159\n",
      "  batch 330 loss: 0.9430010914802551\n",
      "  batch 340 loss: 0.7669797241687775\n",
      "  batch 350 loss: 0.4736734203994274\n",
      "  batch 360 loss: 0.6343325078487396\n",
      "  batch 370 loss: 0.5936789646744728\n",
      "  batch 380 loss: 0.6306924588978291\n",
      "  batch 390 loss: 0.5468327462673187\n",
      "  batch 400 loss: 0.6724075064063072\n",
      "  batch 410 loss: 0.5235129714012146\n",
      "  batch 420 loss: 0.3856322571635246\n",
      "  batch 430 loss: 0.4340425059199333\n",
      "  batch 440 loss: 0.5250230371952057\n",
      "  batch 450 loss: 0.5143049329519271\n",
      "  batch 460 loss: 0.5064067672938108\n",
      "  batch 470 loss: 0.5608719915151597\n",
      "  batch 480 loss: 0.427315266430378\n",
      "  batch 490 loss: 0.6090036764740944\n",
      "  batch 500 loss: 0.439878848195076\n",
      "  batch 510 loss: 0.46839956343173983\n",
      "  batch 520 loss: 0.6205023616552353\n",
      "  batch 530 loss: 0.5295603424310684\n",
      "  batch 540 loss: 0.5100525140762329\n",
      "  batch 550 loss: 0.4160487502813339\n",
      "  batch 560 loss: 0.33055791854858396\n",
      "  batch 570 loss: 0.4398341134190559\n",
      "  batch 580 loss: 0.419683363288641\n",
      "  batch 590 loss: 0.5151958793401719\n",
      "  batch 600 loss: 0.2877049587666988\n",
      "  batch 610 loss: 0.35871549528092145\n",
      "  batch 620 loss: 0.4293813481926918\n",
      "  batch 630 loss: 0.2262368083000183\n",
      "  batch 640 loss: 0.43772358894348146\n",
      "  batch 650 loss: 0.42484743148088455\n",
      "  batch 660 loss: 0.34426167234778404\n",
      "  batch 670 loss: 0.24663300178945063\n",
      "  batch 680 loss: 0.24921292439103127\n",
      "  batch 690 loss: 0.5056853465735912\n",
      "  batch 700 loss: 0.3942547395825386\n",
      "  batch 710 loss: 0.2811485655605793\n",
      "  batch 720 loss: 0.31659617871046064\n",
      "  batch 730 loss: 0.4157093994319439\n",
      "  batch 740 loss: 0.3218838140368462\n",
      "  batch 750 loss: 0.32349951937794685\n",
      "  batch 760 loss: 0.4276246950030327\n",
      "  batch 770 loss: 0.4613655284047127\n",
      "  batch 780 loss: 0.45408571437001227\n",
      "  batch 790 loss: 0.3805233627557755\n",
      "  batch 800 loss: 0.35817993655800817\n",
      "  batch 810 loss: 0.2620824195444584\n",
      "  batch 820 loss: 0.5326488725841045\n",
      "  batch 830 loss: 0.34875828623771665\n",
      "  batch 840 loss: 0.36604894027113916\n",
      "  batch 850 loss: 0.34408671259880064\n",
      "  batch 860 loss: 0.3055058181285858\n",
      "  batch 870 loss: 0.5709414035081863\n",
      "  batch 880 loss: 0.28761468157172204\n",
      "  batch 890 loss: 0.2284127429127693\n",
      "  batch 900 loss: 0.24430876672267915\n",
      "  batch 910 loss: 0.20587398260831832\n",
      "  batch 920 loss: 0.27789096534252167\n",
      "  batch 930 loss: 0.474538654088974\n",
      "  batch 940 loss: 0.2840939797461033\n",
      "  batch 950 loss: 0.3130641058087349\n",
      "  batch 960 loss: 0.35941867902874947\n",
      "  batch 970 loss: 0.3120637908577919\n",
      "  batch 980 loss: 0.3248642697930336\n",
      "  batch 990 loss: 0.31372514739632607\n",
      "  batch 1000 loss: 0.3105937957763672\n",
      "LOSS train 0.3105937957763672 valid 0.9641261100769043\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.3548589263111353\n",
      "  batch 20 loss: 0.36850832104682923\n",
      "  batch 30 loss: 0.45052530616521835\n",
      "  batch 40 loss: 0.21821379624307155\n",
      "  batch 50 loss: 0.22146870754659176\n",
      "  batch 60 loss: 0.18264880925416946\n",
      "  batch 70 loss: 0.3544472839683294\n",
      "  batch 80 loss: 0.3998418774455786\n",
      "  batch 90 loss: 0.33527932688593864\n",
      "  batch 100 loss: 0.28887866884469987\n",
      "  batch 110 loss: 0.2571473967283964\n",
      "  batch 120 loss: 0.21392830610275268\n",
      "  batch 130 loss: 0.33370945006608965\n",
      "  batch 140 loss: 0.12511007413268088\n",
      "  batch 150 loss: 0.23073720633983613\n",
      "  batch 160 loss: 0.23173357211053372\n",
      "  batch 170 loss: 0.3454885441809893\n",
      "  batch 180 loss: 0.3352680720388889\n",
      "  batch 190 loss: 0.2533962519839406\n",
      "  batch 200 loss: 0.26758765429258347\n",
      "  batch 210 loss: 0.1725202538073063\n",
      "  batch 220 loss: 0.1367381051182747\n",
      "  batch 230 loss: 0.1642610602080822\n",
      "  batch 240 loss: 0.2832955472171307\n",
      "  batch 250 loss: 0.19390930710360407\n",
      "  batch 260 loss: 0.21761359162628652\n",
      "  batch 270 loss: 0.1973393613472581\n",
      "  batch 280 loss: 0.1359004994854331\n",
      "  batch 290 loss: 0.21893457025289537\n",
      "  batch 300 loss: 0.35217075124382974\n",
      "  batch 310 loss: 0.17723860489204527\n",
      "  batch 320 loss: 0.14244863679632544\n",
      "  batch 330 loss: 0.26733341813087463\n",
      "  batch 340 loss: 0.13439669348299504\n",
      "  batch 350 loss: 0.25052255131304263\n",
      "  batch 360 loss: 0.16822461634874344\n",
      "  batch 370 loss: 0.27655327850952743\n",
      "  batch 380 loss: 0.14462260603904725\n",
      "  batch 390 loss: 0.10191578110679984\n",
      "  batch 400 loss: 0.21911715250462294\n",
      "  batch 410 loss: 0.18949838504195213\n",
      "  batch 420 loss: 0.21300846226513387\n",
      "  batch 430 loss: 0.3368486754596233\n",
      "  batch 440 loss: 0.2627161007374525\n",
      "  batch 450 loss: 0.15870907790958882\n",
      "  batch 460 loss: 0.2804580360651016\n",
      "  batch 470 loss: 0.17481793761253356\n",
      "  batch 480 loss: 0.1706039546057582\n",
      "  batch 490 loss: 0.23156248750165104\n",
      "  batch 500 loss: 0.25647733844816684\n",
      "  batch 510 loss: 0.13896135985851288\n",
      "  batch 520 loss: 0.2722804445773363\n",
      "  batch 530 loss: 0.1347854919731617\n",
      "  batch 540 loss: 0.22518462240695952\n",
      "  batch 550 loss: 0.08378555374220013\n",
      "  batch 560 loss: 0.20259951427578926\n",
      "  batch 570 loss: 0.10215765051543713\n",
      "  batch 580 loss: 0.14824394965544344\n",
      "  batch 590 loss: 0.12471249625086785\n",
      "  batch 600 loss: 0.077910678088665\n",
      "  batch 610 loss: 0.17448247745633125\n",
      "  batch 620 loss: 0.16967092100530862\n",
      "  batch 630 loss: 0.32759261056780814\n",
      "  batch 640 loss: 0.1671552438288927\n",
      "  batch 650 loss: 0.33932774076238276\n",
      "  batch 660 loss: 0.1246006403118372\n",
      "  batch 670 loss: 0.1009953677188605\n",
      "  batch 680 loss: 0.10479362308979034\n",
      "  batch 690 loss: 0.14728995356708766\n",
      "  batch 700 loss: 0.10042911320924759\n",
      "  batch 710 loss: 0.24776062443852426\n",
      "  batch 720 loss: 0.23014068156480788\n",
      "  batch 730 loss: 0.09371924195438623\n",
      "  batch 740 loss: 0.11378416898660362\n",
      "  batch 750 loss: 0.1069579373113811\n",
      "  batch 760 loss: 0.3263426825404167\n",
      "  batch 770 loss: 0.2357343275099993\n",
      "  batch 780 loss: 0.11983388476073742\n",
      "  batch 790 loss: 0.272469500079751\n",
      "  batch 800 loss: 0.2537389824166894\n",
      "  batch 810 loss: 0.12559081856161355\n",
      "  batch 820 loss: 0.16153124016709625\n",
      "  batch 830 loss: 0.18386099115014076\n",
      "  batch 840 loss: 0.2890265878289938\n",
      "  batch 850 loss: 0.1754987814463675\n",
      "  batch 860 loss: 0.17672945819795133\n",
      "  batch 870 loss: 0.21030097296461464\n",
      "  batch 880 loss: 0.10979823246598244\n",
      "  batch 890 loss: 0.20505988802760838\n",
      "  batch 900 loss: 0.1653234103694558\n",
      "  batch 910 loss: 0.18297140821814536\n",
      "  batch 920 loss: 0.13162965849041938\n",
      "  batch 930 loss: 0.04488780860556289\n",
      "  batch 940 loss: 0.2315103451255709\n",
      "  batch 950 loss: 0.1756728461012244\n",
      "  batch 960 loss: 0.30859694415703415\n",
      "  batch 970 loss: 0.17084147706627845\n",
      "  batch 980 loss: 0.15878557246178387\n",
      "  batch 990 loss: 0.12358570704236627\n",
      "  batch 1000 loss: 0.1112430983223021\n",
      "LOSS train 0.1112430983223021 valid 0.537959635257721\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.18727493695914746\n",
      "  batch 20 loss: 0.11985926926136017\n",
      "  batch 30 loss: 0.06311291838064789\n",
      "  batch 40 loss: 0.1475685451179743\n",
      "  batch 50 loss: 0.1496900450438261\n",
      "  batch 60 loss: 0.15861817821860313\n",
      "  batch 70 loss: 0.060967539343982934\n",
      "  batch 80 loss: 0.11108129802159965\n",
      "  batch 90 loss: 0.11860810574144125\n",
      "  batch 100 loss: 0.10234403600916267\n",
      "  batch 110 loss: 0.06240887651219964\n",
      "  batch 120 loss: 0.2547274161595851\n",
      "  batch 130 loss: 0.17269961033016443\n",
      "  batch 140 loss: 0.14031684566289188\n",
      "  batch 150 loss: 0.43380832066759467\n",
      "  batch 160 loss: 0.3060025040060282\n",
      "  batch 170 loss: 0.058041926566511394\n",
      "  batch 180 loss: 0.1257519408594817\n",
      "  batch 190 loss: 0.2681971203535795\n",
      "  batch 200 loss: 0.069721519947052\n",
      "  batch 210 loss: 0.16418087035417556\n",
      "  batch 220 loss: 0.06349653005599976\n",
      "  batch 230 loss: 0.1371136985719204\n",
      "  batch 240 loss: 0.1662404527887702\n",
      "  batch 250 loss: 0.07807708363980055\n",
      "  batch 260 loss: 0.046109176916070284\n",
      "  batch 270 loss: 0.28548389626666903\n",
      "  batch 280 loss: 0.18933022897690535\n",
      "  batch 290 loss: 0.12581814001314343\n",
      "  batch 300 loss: 0.16353032737970352\n",
      "  batch 310 loss: 0.10397975416854024\n",
      "  batch 320 loss: 0.05681107775308192\n",
      "  batch 330 loss: 0.13023260112386198\n",
      "  batch 340 loss: 0.1018932580947876\n",
      "  batch 350 loss: 0.18027809909544884\n",
      "  batch 360 loss: 0.061358231119811535\n",
      "  batch 370 loss: 0.26935528190806507\n",
      "  batch 380 loss: 0.1743867378681898\n",
      "  batch 390 loss: 0.09624091703444719\n",
      "  batch 400 loss: 0.18173570428043603\n",
      "  batch 410 loss: 0.13404982993379236\n",
      "  batch 420 loss: 0.1708738728426397\n",
      "  batch 430 loss: 0.07073446344584226\n",
      "  batch 440 loss: 0.06772369854152202\n",
      "  batch 450 loss: 0.08307114103808999\n",
      "  batch 460 loss: 0.1085194860585034\n",
      "  batch 470 loss: 0.04687359174713492\n",
      "  batch 480 loss: 0.045359413139522076\n",
      "  batch 490 loss: 0.053195411525666716\n",
      "  batch 500 loss: 0.22265677265822886\n",
      "  batch 510 loss: 0.13036484085023403\n",
      "  batch 520 loss: 0.22903198162093757\n",
      "  batch 530 loss: 0.19244940727949142\n",
      "  batch 540 loss: 0.1479297060519457\n",
      "  batch 550 loss: 0.15290111768990755\n",
      "  batch 560 loss: 0.24059137813746928\n",
      "  batch 570 loss: 0.13444357514381408\n",
      "  batch 580 loss: 0.1355081803165376\n",
      "  batch 590 loss: 0.03458959944546223\n",
      "  batch 600 loss: 0.11405916986986994\n",
      "  batch 610 loss: 0.046702088881284\n",
      "  batch 620 loss: 0.068449667096138\n",
      "  batch 630 loss: 0.07670526802539826\n",
      "  batch 640 loss: 0.13768295370973646\n",
      "  batch 650 loss: 0.05948627397883684\n",
      "  batch 660 loss: 0.07286194209009408\n",
      "  batch 670 loss: 0.1919772993773222\n",
      "  batch 680 loss: 0.10962488502264023\n",
      "  batch 690 loss: 0.2058648444712162\n",
      "  batch 700 loss: 0.1704457089304924\n",
      "  batch 710 loss: 0.08787115551531315\n",
      "  batch 720 loss: 0.07045653313398362\n",
      "  batch 730 loss: 0.07227567983791232\n",
      "  batch 740 loss: 0.03044793214648962\n",
      "  batch 750 loss: 0.037542882421985266\n",
      "  batch 760 loss: 0.033564640767872335\n",
      "  batch 770 loss: 0.12082073609344661\n",
      "  batch 780 loss: 0.13220420507714153\n",
      "  batch 790 loss: 0.06992908688262105\n",
      "  batch 800 loss: 0.029006437223870307\n",
      "  batch 810 loss: 0.07241522342665122\n",
      "  batch 820 loss: 0.09471423085778952\n",
      "  batch 830 loss: 0.09324777126312256\n",
      "  batch 840 loss: 0.03850568747147918\n",
      "  batch 850 loss: 0.05886103166267276\n",
      "  batch 860 loss: 0.28079559123143555\n",
      "  batch 870 loss: 0.07291282783262432\n",
      "  batch 880 loss: 0.1094238446559757\n",
      "  batch 890 loss: 0.07839382044039667\n",
      "  batch 900 loss: 0.11881994381546974\n",
      "  batch 910 loss: 0.04847670709714293\n",
      "  batch 920 loss: 0.03886463963426649\n",
      "  batch 930 loss: 0.05953087536618114\n",
      "  batch 940 loss: 0.13387333750724792\n",
      "  batch 950 loss: 0.08710525603964925\n",
      "  batch 960 loss: 0.09754531588405371\n",
      "  batch 970 loss: 0.028892306331545114\n",
      "  batch 980 loss: 0.0488698105327785\n",
      "  batch 990 loss: 0.07848827154375612\n",
      "  batch 1000 loss: 0.0773068645503372\n",
      "LOSS train 0.0773068645503372 valid 0.28818684816360474\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.04771576598286629\n",
      "  batch 20 loss: 0.09445168175734579\n",
      "  batch 30 loss: 0.20875820559449493\n",
      "  batch 40 loss: 0.12365393210202455\n",
      "  batch 50 loss: 0.14887401056475938\n",
      "  batch 60 loss: 0.048427986819297074\n",
      "  batch 70 loss: 0.157813908578828\n",
      "  batch 80 loss: 0.14935810794122517\n",
      "  batch 90 loss: 0.06301901047118008\n",
      "  batch 100 loss: 0.07309079840779305\n",
      "  batch 110 loss: 0.13916041404008866\n",
      "  batch 120 loss: 0.14476702357642351\n",
      "  batch 130 loss: 0.2745853628963232\n",
      "  batch 140 loss: 0.17227872163057328\n",
      "  batch 150 loss: 0.1055861002765596\n",
      "  batch 160 loss: 0.06077141310088337\n",
      "  batch 170 loss: 0.1118971798568964\n",
      "  batch 180 loss: 0.05281486553139984\n",
      "  batch 190 loss: 0.06360354227945209\n",
      "  batch 200 loss: 0.07221081145107747\n",
      "  batch 210 loss: 0.07405779303517193\n",
      "  batch 220 loss: 0.027760434616357087\n",
      "  batch 230 loss: 0.2137677415041253\n",
      "  batch 240 loss: 0.09995691990479827\n",
      "  batch 250 loss: 0.13836205502739177\n",
      "  batch 260 loss: 0.08765477370470762\n",
      "  batch 270 loss: 0.09896264965645969\n",
      "  batch 280 loss: 0.11440997500903904\n",
      "  batch 290 loss: 0.05161716472357512\n",
      "  batch 300 loss: 0.07867253939621151\n",
      "  batch 310 loss: 0.050100954528898\n",
      "  batch 320 loss: 0.159118390083313\n",
      "  batch 330 loss: 0.2536777066066861\n",
      "  batch 340 loss: 0.13300906852819025\n",
      "  batch 350 loss: 0.0567130459472537\n",
      "  batch 360 loss: 0.09018463478423655\n",
      "  batch 370 loss: 0.09400780508294701\n",
      "  batch 380 loss: 0.1298670777352527\n",
      "  batch 390 loss: 0.0921708223875612\n",
      "  batch 400 loss: 0.038201764051336795\n",
      "  batch 410 loss: 0.053012996388133615\n",
      "  batch 420 loss: 0.040018001664429904\n",
      "  batch 430 loss: 0.015582039020955563\n",
      "  batch 440 loss: 0.18594255219213665\n",
      "  batch 450 loss: 0.28054643732029944\n",
      "  batch 460 loss: 0.12447241632035003\n",
      "  batch 470 loss: 0.12636534180492162\n",
      "  batch 480 loss: 0.09063105238601565\n",
      "  batch 490 loss: 0.1107519987039268\n",
      "  batch 500 loss: 0.27039550195913764\n",
      "  batch 510 loss: 0.11241774372756481\n",
      "  batch 520 loss: 0.06821379417087883\n",
      "  batch 530 loss: 0.09564707439858466\n",
      "  batch 540 loss: 0.07278535363730043\n",
      "  batch 550 loss: 0.10217269249260426\n",
      "  batch 560 loss: 0.0743989463080652\n",
      "  batch 570 loss: 0.08389940471388399\n",
      "  batch 580 loss: 0.04100194941274822\n",
      "  batch 590 loss: 0.07841142416000366\n",
      "  batch 600 loss: 0.13669106010347604\n",
      "  batch 610 loss: 0.03540874712634832\n",
      "  batch 620 loss: 0.04781816371250898\n",
      "  batch 630 loss: 0.032135773357003926\n",
      "  batch 640 loss: 0.07898603389039635\n",
      "  batch 650 loss: 0.05035073256003671\n",
      "  batch 660 loss: 0.08242369550280274\n",
      "  batch 670 loss: 0.042565380316227676\n",
      "  batch 680 loss: 0.014598554279655219\n",
      "  batch 690 loss: 0.029888322786428036\n",
      "  batch 700 loss: 0.06968786832876503\n",
      "  batch 710 loss: 0.1170297162141651\n",
      "  batch 720 loss: 0.057337440480478105\n",
      "  batch 730 loss: 0.10761951108579523\n",
      "  batch 740 loss: 0.1408041076734662\n",
      "  batch 750 loss: 0.137260094541125\n",
      "  batch 760 loss: 0.2059605997055769\n",
      "  batch 770 loss: 0.0910092486999929\n",
      "  batch 780 loss: 0.07169298804365098\n",
      "  batch 790 loss: 0.1228066322975792\n",
      "  batch 800 loss: 0.053905321704223755\n",
      "  batch 810 loss: 0.02666193465702236\n",
      "  batch 820 loss: 0.1376889503095299\n",
      "  batch 830 loss: 0.25648795403540137\n",
      "  batch 840 loss: 0.09811591701582074\n",
      "  batch 850 loss: 0.05242099941242486\n",
      "  batch 860 loss: 0.02844993178732693\n",
      "  batch 870 loss: 0.11947566894814372\n",
      "  batch 880 loss: 0.042023904900997874\n",
      "  batch 890 loss: 0.07358202980831266\n",
      "  batch 900 loss: 0.04620598703622818\n",
      "  batch 910 loss: 0.012248474458465353\n",
      "  batch 920 loss: 0.05292056249163579\n",
      "  batch 930 loss: 0.0633000187575817\n",
      "  batch 940 loss: 0.04294990180060267\n",
      "  batch 950 loss: 0.092446613823995\n",
      "  batch 960 loss: 0.047036606451729315\n",
      "  batch 970 loss: 0.1736664680764079\n",
      "  batch 980 loss: 0.05900138551369309\n",
      "  batch 990 loss: 0.06225552447140217\n",
      "  batch 1000 loss: 0.019819313613697886\n",
      "LOSS train 0.019819313613697886 valid 0.5020517110824585\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.06061989918525797\n",
      "  batch 20 loss: 0.12362766654696315\n",
      "  batch 30 loss: 0.28365539917722343\n",
      "  batch 40 loss: 0.12379048215225338\n",
      "  batch 50 loss: 0.04550378554267809\n",
      "  batch 60 loss: 0.030980469746282324\n",
      "  batch 70 loss: 0.012121629103785381\n",
      "  batch 80 loss: 0.11787239639088512\n",
      "  batch 90 loss: 0.05968810296617448\n",
      "  batch 100 loss: 0.06613823322113603\n",
      "  batch 110 loss: 0.18825876279734075\n",
      "  batch 120 loss: 0.05195780945941806\n",
      "  batch 130 loss: 0.06641449112212286\n",
      "  batch 140 loss: 0.37431767778471114\n",
      "  batch 150 loss: 0.1301888287096517\n",
      "  batch 160 loss: 0.13256420540274122\n",
      "  batch 170 loss: 0.03003595576155931\n",
      "  batch 180 loss: 0.07723221525084227\n",
      "  batch 190 loss: 0.09596865606727079\n",
      "  batch 200 loss: 0.13148066587746143\n",
      "  batch 210 loss: 0.02708543138578534\n",
      "  batch 220 loss: 0.07466223193332552\n",
      "  batch 230 loss: 0.04929636735469103\n",
      "  batch 240 loss: 0.06004913931246847\n",
      "  batch 250 loss: 0.032654107292182745\n",
      "  batch 260 loss: 0.06406839460832998\n",
      "  batch 270 loss: 0.04673584832344204\n",
      "  batch 280 loss: 0.04654447985813022\n",
      "  batch 290 loss: 0.10900633258279413\n",
      "  batch 300 loss: 0.03180819125846028\n",
      "  batch 310 loss: 0.049533133115619424\n",
      "  batch 320 loss: 0.0683294540271163\n",
      "  batch 330 loss: 0.10093939150683581\n",
      "  batch 340 loss: 0.0966167276725173\n",
      "  batch 350 loss: 0.042386878095567224\n",
      "  batch 360 loss: 0.049770980165340005\n",
      "  batch 370 loss: 0.028474391077179462\n",
      "  batch 380 loss: 0.18025858256733046\n",
      "  batch 390 loss: 0.02076991000212729\n",
      "  batch 400 loss: 0.24402654254809023\n",
      "  batch 410 loss: 0.024451766908168793\n",
      "  batch 420 loss: 0.07094510719180107\n",
      "  batch 430 loss: 0.028473176527768373\n",
      "  batch 440 loss: 0.06079095667228103\n",
      "  batch 450 loss: 0.013760527037084103\n",
      "  batch 460 loss: 0.08719708805438131\n",
      "  batch 470 loss: 0.10339804502436892\n",
      "  batch 480 loss: 0.009079732117243111\n",
      "  batch 490 loss: 0.1447525500319898\n",
      "  batch 500 loss: 0.015868602332193404\n",
      "  batch 510 loss: 0.029778210166841745\n",
      "  batch 520 loss: 0.03977608409477398\n",
      "  batch 530 loss: 0.04018911775201559\n",
      "  batch 540 loss: 0.015478270780295133\n",
      "  batch 550 loss: 0.022321633668616413\n",
      "  batch 560 loss: 0.025182536570355296\n",
      "  batch 570 loss: 0.04852123765740544\n",
      "  batch 580 loss: 0.13189692148007454\n",
      "  batch 590 loss: 0.034165833774022755\n",
      "  batch 600 loss: 0.030812233267351986\n",
      "  batch 610 loss: 0.02140996130183339\n",
      "  batch 620 loss: 0.07758025595685467\n",
      "  batch 630 loss: 0.019643040327355265\n",
      "  batch 640 loss: 0.0072622614912688736\n",
      "  batch 650 loss: 0.18218171613989398\n",
      "  batch 660 loss: 0.11346651297062635\n",
      "  batch 670 loss: 0.1420622531324625\n",
      "  batch 680 loss: 0.054245449043810366\n",
      "  batch 690 loss: 0.242357322818134\n",
      "  batch 700 loss: 0.17530533466488124\n",
      "  batch 710 loss: 0.10568442170042544\n",
      "  batch 720 loss: 0.2846258774487069\n",
      "  batch 730 loss: 0.038997712143464015\n",
      "  batch 740 loss: 0.11170149599201977\n",
      "  batch 750 loss: 0.0412334811873734\n",
      "  batch 760 loss: 0.09388361508026719\n",
      "  batch 770 loss: 0.054634285531938075\n",
      "  batch 780 loss: 0.020267042471095918\n",
      "  batch 790 loss: 0.13321576126618312\n",
      "  batch 800 loss: 0.06672090570209548\n",
      "  batch 810 loss: 0.029881495039444417\n",
      "  batch 820 loss: 0.010787134268321097\n",
      "  batch 830 loss: 0.02585966532351449\n",
      "  batch 840 loss: 0.05925086851348169\n",
      "  batch 850 loss: 0.01252949156332761\n",
      "  batch 860 loss: 0.026261802529916167\n",
      "  batch 870 loss: 0.055708504130598156\n",
      "  batch 880 loss: 0.05486296406015754\n",
      "  batch 890 loss: 0.14160035587847233\n",
      "  batch 900 loss: 0.13582800459116698\n",
      "  batch 910 loss: 0.21368629839271308\n",
      "  batch 920 loss: 0.026243774086469784\n",
      "  batch 930 loss: 0.04463976928673219\n",
      "  batch 940 loss: 0.05234308540238999\n",
      "  batch 950 loss: 0.06526163993403315\n",
      "  batch 960 loss: 0.07242592909606174\n",
      "  batch 970 loss: 0.13369777873158456\n",
      "  batch 980 loss: 0.05284014886710793\n",
      "  batch 990 loss: 0.23069187949877232\n",
      "  batch 1000 loss: 0.10537738529965282\n",
      "LOSS train 0.10537738529965282 valid 0.29846614599227905\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.14502856167964637\n",
      "  batch 20 loss: 0.06304254551650956\n",
      "  batch 30 loss: 0.02432905831374228\n",
      "  batch 40 loss: 0.07530002552084625\n",
      "  batch 50 loss: 0.022098092129454017\n",
      "  batch 60 loss: 0.059643250820226965\n",
      "  batch 70 loss: 0.14696756792254745\n",
      "  batch 80 loss: 0.19819505584891886\n",
      "  batch 90 loss: 0.0480489692883566\n",
      "  batch 100 loss: 0.018269403278827666\n",
      "  batch 110 loss: 0.005595197749789805\n",
      "  batch 120 loss: 0.07961734445998445\n",
      "  batch 130 loss: 0.11222381787374616\n",
      "  batch 140 loss: 0.026858608052134515\n",
      "  batch 150 loss: 0.020400887314463035\n",
      "  batch 160 loss: 0.10740096312947571\n",
      "  batch 170 loss: 0.07524042539298534\n",
      "  batch 180 loss: 0.02063054065220058\n",
      "  batch 190 loss: 0.06492250684386817\n",
      "  batch 200 loss: 0.014757557271514088\n",
      "  batch 210 loss: 0.026232856314163656\n",
      "  batch 220 loss: 0.007727941672783345\n",
      "  batch 230 loss: 0.013687060057418422\n",
      "  batch 240 loss: 0.11788501492701471\n",
      "  batch 250 loss: 0.05897028809413314\n",
      "  batch 260 loss: 0.0187210900709033\n",
      "  batch 270 loss: 0.035648296575527635\n",
      "  batch 280 loss: 0.015648533822968602\n",
      "  batch 290 loss: 0.0403336706571281\n",
      "  batch 300 loss: 0.01251636209199205\n",
      "  batch 310 loss: 0.032061667763628066\n",
      "  batch 320 loss: 0.02185904422076419\n",
      "  batch 330 loss: 0.13291338218841703\n",
      "  batch 340 loss: 0.019444234040565788\n",
      "  batch 350 loss: 0.012660089624114334\n",
      "  batch 360 loss: 0.02307443358004093\n",
      "  batch 370 loss: 0.02354659599950537\n",
      "  batch 380 loss: 0.019312621699646114\n",
      "  batch 390 loss: 0.11661078473553062\n",
      "  batch 400 loss: 0.030685491021722557\n",
      "  batch 410 loss: 0.10515094075817615\n",
      "  batch 420 loss: 0.08105545302387326\n",
      "  batch 430 loss: 0.139971362054348\n",
      "  batch 440 loss: 0.1724673298187554\n",
      "  batch 450 loss: 0.19805274582467974\n",
      "  batch 460 loss: 0.01228212711866945\n",
      "  batch 470 loss: 0.0577599459560588\n",
      "  batch 480 loss: 0.020760914823040367\n",
      "  batch 490 loss: 0.04810587201500312\n",
      "  batch 500 loss: 0.11666360255330802\n",
      "  batch 510 loss: 0.2224863726645708\n",
      "  batch 520 loss: 0.12493525249883533\n",
      "  batch 530 loss: 0.0673488184780581\n",
      "  batch 540 loss: 0.013774272962473333\n",
      "  batch 550 loss: 0.016356983536388724\n",
      "  batch 560 loss: 0.027122819202486426\n",
      "  batch 570 loss: 0.02571016582660377\n",
      "  batch 580 loss: 0.023737091716611758\n",
      "  batch 590 loss: 0.03254531514830887\n",
      "  batch 600 loss: 0.07705126685323194\n",
      "  batch 610 loss: 0.02376968339085579\n",
      "  batch 620 loss: 0.12538479859940707\n",
      "  batch 630 loss: 0.08184145241975785\n",
      "  batch 640 loss: 0.11057951324619353\n",
      "  batch 650 loss: 0.1340932029299438\n",
      "  batch 660 loss: 0.07567298294598004\n",
      "  batch 670 loss: 0.12397774271667003\n",
      "  batch 680 loss: 0.041951130249071863\n",
      "  batch 690 loss: 0.07469199932529591\n",
      "  batch 700 loss: 0.046784755136468445\n",
      "  batch 710 loss: 0.04489779016003013\n",
      "  batch 720 loss: 0.023661611462011935\n",
      "  batch 730 loss: 0.17226181391160936\n",
      "  batch 740 loss: 0.042456812458112836\n",
      "  batch 750 loss: 0.005119853632641025\n",
      "  batch 760 loss: 0.10168386233126511\n",
      "  batch 770 loss: 0.11706827720045113\n",
      "  batch 780 loss: 0.13410137007012962\n",
      "  batch 790 loss: 0.12338738646358252\n",
      "  batch 800 loss: 0.06283183908672071\n",
      "  batch 810 loss: 0.012527379563107389\n",
      "  batch 820 loss: 0.09980570673069451\n",
      "  batch 830 loss: 0.033364312583580615\n",
      "  batch 840 loss: 0.026389231020584703\n",
      "  batch 850 loss: 0.01141589500475675\n",
      "  batch 860 loss: 0.044338923165923916\n",
      "  batch 870 loss: 0.017755047080572694\n",
      "  batch 880 loss: 0.042977958964183924\n",
      "  batch 890 loss: 0.032859066408127545\n",
      "  batch 900 loss: 0.0233122187666595\n",
      "  batch 910 loss: 0.02758228003513068\n",
      "  batch 920 loss: 0.017408727039583027\n",
      "  batch 930 loss: 0.0038172908447450028\n",
      "  batch 940 loss: 0.0113439733046107\n",
      "  batch 950 loss: 0.04216701097902842\n",
      "  batch 960 loss: 0.032441864773863925\n",
      "  batch 970 loss: 0.019612912368029357\n",
      "  batch 980 loss: 0.013745599274989218\n",
      "  batch 990 loss: 0.05451236674562097\n",
      "  batch 1000 loss: 0.020697409147396685\n",
      "LOSS train 0.020697409147396685 valid 0.21253834664821625\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.029849169891531347\n",
      "  batch 20 loss: 0.0041822960542049255\n",
      "  batch 30 loss: 0.011864476450136862\n",
      "  batch 40 loss: 0.16792496794951148\n",
      "  batch 50 loss: 0.2862225312739611\n",
      "  batch 60 loss: 0.08049775979598053\n",
      "  batch 70 loss: 0.0643866635626182\n",
      "  batch 80 loss: 0.03847614721744321\n",
      "  batch 90 loss: 0.09970688258763402\n",
      "  batch 100 loss: 0.05358846877934411\n",
      "  batch 110 loss: 0.015411800549190956\n",
      "  batch 120 loss: 0.003293566731736064\n",
      "  batch 130 loss: 0.023238784330897034\n",
      "  batch 140 loss: 0.009657082660123707\n",
      "  batch 150 loss: 0.05941708302125335\n",
      "  batch 160 loss: 0.019424625486135483\n",
      "  batch 170 loss: 0.02582614168059081\n",
      "  batch 180 loss: 0.08984276290866547\n",
      "  batch 190 loss: 0.0993897486012429\n",
      "  batch 200 loss: 0.014299466481315904\n",
      "  batch 210 loss: 0.12140915935160593\n",
      "  batch 220 loss: 0.04724723054096103\n",
      "  batch 230 loss: 0.026018640771508218\n",
      "  batch 240 loss: 0.037727116444148126\n",
      "  batch 250 loss: 0.033113917481387034\n",
      "  batch 260 loss: 0.09852184717892669\n",
      "  batch 270 loss: 0.054668973991647364\n",
      "  batch 280 loss: 0.04014713200740516\n",
      "  batch 290 loss: 0.198127116530668\n",
      "  batch 300 loss: 0.062110622914042325\n",
      "  batch 310 loss: 0.0409789988421835\n",
      "  batch 320 loss: 0.13777327991556376\n",
      "  batch 330 loss: 0.16879894831217826\n",
      "  batch 340 loss: 0.04676533527672291\n",
      "  batch 350 loss: 0.027013596252072603\n",
      "  batch 360 loss: 0.009617226826958359\n",
      "  batch 370 loss: 0.017330892977770417\n",
      "  batch 380 loss: 0.018368044946691954\n",
      "  batch 390 loss: 0.013636433193460107\n",
      "  batch 400 loss: 0.006708889815490693\n",
      "  batch 410 loss: 0.0476622937596403\n",
      "  batch 420 loss: 0.05019505314994603\n",
      "  batch 430 loss: 0.017033613519743085\n",
      "  batch 440 loss: 0.014148596627637743\n",
      "  batch 450 loss: 0.2577175713900942\n",
      "  batch 460 loss: 0.040757424244657156\n",
      "  batch 470 loss: 0.005698835389921442\n",
      "  batch 480 loss: 0.09580766608705744\n",
      "  batch 490 loss: 0.046207946818321945\n",
      "  batch 500 loss: 0.003717420854809461\n",
      "  batch 510 loss: 0.00220481044088956\n",
      "  batch 520 loss: 0.024122079610242507\n",
      "  batch 530 loss: 0.005250596429686993\n",
      "  batch 540 loss: 0.055488347067148426\n",
      "  batch 550 loss: 0.043685717004700565\n",
      "  batch 560 loss: 0.06173913583625108\n",
      "  batch 570 loss: 0.01872703025001101\n",
      "  batch 580 loss: 0.02171137760160491\n",
      "  batch 590 loss: 0.009897200600244105\n",
      "  batch 600 loss: 0.016750903474166988\n",
      "  batch 610 loss: 0.03330027497722767\n",
      "  batch 620 loss: 0.11499734590761364\n",
      "  batch 630 loss: 0.019673521007644013\n",
      "  batch 640 loss: 0.004019006586167962\n",
      "  batch 650 loss: 0.011946981580695137\n",
      "  batch 660 loss: 0.019082760025048627\n",
      "  batch 670 loss: 0.0050057126383762805\n",
      "  batch 680 loss: 0.00722182032186538\n",
      "  batch 690 loss: 0.1494756066473201\n",
      "  batch 700 loss: 0.09364378014579415\n",
      "  batch 710 loss: 0.01274513595053577\n",
      "  batch 720 loss: 0.002731671676156111\n",
      "  batch 730 loss: 0.04039580649696291\n",
      "  batch 740 loss: 0.025278556291596033\n",
      "  batch 750 loss: 0.006808178286883048\n",
      "  batch 760 loss: 0.0024379959984798917\n",
      "  batch 770 loss: 0.11556072692619637\n",
      "  batch 780 loss: 0.03863265195395797\n",
      "  batch 790 loss: 0.05890646302141249\n",
      "  batch 800 loss: 0.01174929910339415\n",
      "  batch 810 loss: 0.00609371749451384\n",
      "  batch 820 loss: 0.005339265956718009\n",
      "  batch 830 loss: 0.02579982636962086\n",
      "  batch 840 loss: 0.01241089126560837\n",
      "  batch 850 loss: 0.011884235520847141\n",
      "  batch 860 loss: 0.15764080386143178\n",
      "  batch 870 loss: 0.012172232242301106\n",
      "  batch 880 loss: 0.017311592714395373\n",
      "  batch 890 loss: 0.1312805361347273\n",
      "  batch 900 loss: 0.01896387482411228\n",
      "  batch 910 loss: 0.010228879237547516\n",
      "  batch 920 loss: 0.02312015371135203\n",
      "  batch 930 loss: 0.009380146296462045\n",
      "  batch 940 loss: 0.020996364287566394\n",
      "  batch 950 loss: 0.0232654228573665\n",
      "  batch 960 loss: 0.02424158364883624\n",
      "  batch 970 loss: 0.002439263818087056\n",
      "  batch 980 loss: 0.017856794712133705\n",
      "  batch 990 loss: 0.033102949464228\n",
      "  batch 1000 loss: 0.08453285669675097\n",
      "LOSS train 0.08453285669675097 valid 0.4249802529811859\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.06915479234812665\n",
      "  batch 20 loss: 0.15394339306512847\n",
      "  batch 30 loss: 0.036338820261880754\n",
      "  batch 40 loss: 0.3386069976113504\n",
      "  batch 50 loss: 0.04710818997118622\n",
      "  batch 60 loss: 0.15104077230207621\n",
      "  batch 70 loss: 0.15979192841332407\n",
      "  batch 80 loss: 0.05613794646051247\n",
      "  batch 90 loss: 0.08470569592900574\n",
      "  batch 100 loss: 0.18256241772323847\n",
      "  batch 110 loss: 0.24985183054268417\n",
      "  batch 120 loss: 0.015288425080507295\n",
      "  batch 130 loss: 0.06559754330664873\n",
      "  batch 140 loss: 0.06998999146744608\n",
      "  batch 150 loss: 0.01353824264369905\n",
      "  batch 160 loss: 0.10960114209447055\n",
      "  batch 170 loss: 0.056800493877381085\n",
      "  batch 180 loss: 0.19968009815202095\n",
      "  batch 190 loss: 0.00977051151858177\n",
      "  batch 200 loss: 0.1130639219423756\n",
      "  batch 210 loss: 0.055513519028318116\n",
      "  batch 220 loss: 0.09478758486220613\n",
      "  batch 230 loss: 0.013483335765704397\n",
      "  batch 240 loss: 0.025742508564144374\n",
      "  batch 250 loss: 0.012319184467196464\n",
      "  batch 260 loss: 0.020172900406760164\n",
      "  batch 270 loss: 0.03150915876030922\n",
      "  batch 280 loss: 0.1608578588347882\n",
      "  batch 290 loss: 0.009518484293948859\n",
      "  batch 300 loss: 0.0027636434504529463\n",
      "  batch 310 loss: 0.020703782234340905\n",
      "  batch 320 loss: 0.02404379058862105\n",
      "  batch 330 loss: 0.012196092982776463\n",
      "  batch 340 loss: 0.017261056567076594\n",
      "  batch 350 loss: 0.0290891337353969\n",
      "  batch 360 loss: 0.15199658737983554\n",
      "  batch 370 loss: 0.19744057734496892\n",
      "  batch 380 loss: 0.021123466780409217\n",
      "  batch 390 loss: 0.004468259797431528\n",
      "  batch 400 loss: 0.010027381614781916\n",
      "  batch 410 loss: 0.05167888936703093\n",
      "  batch 420 loss: 0.029872474097646772\n",
      "  batch 430 loss: 0.01486680789384991\n",
      "  batch 440 loss: 0.07312784600071609\n",
      "  batch 450 loss: 0.030084815018926746\n",
      "  batch 460 loss: 0.09821062278351747\n",
      "  batch 470 loss: 0.1315946472575888\n",
      "  batch 480 loss: 0.18819435518817046\n",
      "  batch 490 loss: 0.17717622165801003\n",
      "  batch 500 loss: 0.023608430099557154\n",
      "  batch 510 loss: 0.20743322564521804\n",
      "  batch 520 loss: 0.04112891326658428\n",
      "  batch 530 loss: 0.109944102820009\n",
      "  batch 540 loss: 0.03164615694186068\n",
      "  batch 550 loss: 0.1611679903464392\n",
      "  batch 560 loss: 0.04547333533409983\n",
      "  batch 570 loss: 0.12197308350587263\n",
      "  batch 580 loss: 0.048833597352495414\n",
      "  batch 590 loss: 0.01132336701703025\n",
      "  batch 600 loss: 0.02184300553635694\n",
      "  batch 610 loss: 0.02069447099347599\n",
      "  batch 620 loss: 0.007213447411777452\n",
      "  batch 630 loss: 0.014449231140315533\n",
      "  batch 640 loss: 0.12970418303739278\n",
      "  batch 650 loss: 0.0810294097289443\n",
      "  batch 660 loss: 0.022661510028410704\n",
      "  batch 670 loss: 0.009163477853871883\n",
      "  batch 680 loss: 0.00416441960260272\n",
      "  batch 690 loss: 0.1593340548221022\n",
      "  batch 700 loss: 0.08695294018834829\n",
      "  batch 710 loss: 0.07946247491199757\n",
      "  batch 720 loss: 0.024977444857358932\n",
      "  batch 730 loss: 0.02242409341270104\n",
      "  batch 740 loss: 0.015415549848694355\n",
      "  batch 750 loss: 0.21105710508418268\n",
      "  batch 760 loss: 0.09098158065462485\n",
      "  batch 770 loss: 0.04552088184282184\n",
      "  batch 780 loss: 0.129596718295943\n",
      "  batch 790 loss: 0.17688560488750227\n",
      "  batch 800 loss: 0.10791219200400519\n",
      "  batch 810 loss: 0.19794735744362696\n",
      "  batch 820 loss: 0.4215732988901436\n",
      "  batch 830 loss: 0.10002887553418986\n",
      "  batch 840 loss: 0.02958764237992\n",
      "  batch 850 loss: 0.0153717958368361\n",
      "  batch 860 loss: 0.01169776589376852\n",
      "  batch 870 loss: 0.061316452478058635\n",
      "  batch 880 loss: 0.032040682027582076\n",
      "  batch 890 loss: 0.05278358617797494\n",
      "  batch 900 loss: 0.011839276168029756\n",
      "  batch 910 loss: 0.022819901950424538\n",
      "  batch 920 loss: 0.008701816166285426\n",
      "  batch 930 loss: 0.09903482176596298\n",
      "  batch 940 loss: 0.044125296943821016\n",
      "  batch 950 loss: 0.00576311070472002\n",
      "  batch 960 loss: 0.013684032438322901\n",
      "  batch 970 loss: 0.015531376452418044\n",
      "  batch 980 loss: 0.019342548109125345\n",
      "  batch 990 loss: 0.02838769053341821\n",
      "  batch 1000 loss: 0.022942887229146437\n",
      "LOSS train 0.022942887229146437 valid 0.2173347920179367\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.026784067577682435\n",
      "  batch 20 loss: 0.007803846165916184\n",
      "  batch 30 loss: 0.00849874918931164\n",
      "  batch 40 loss: 0.013649582833750173\n",
      "  batch 50 loss: 0.010029191321518739\n",
      "  batch 60 loss: 0.01569232001202181\n",
      "  batch 70 loss: 0.005056987382704392\n",
      "  batch 80 loss: 0.00497647799202241\n",
      "  batch 90 loss: 0.07180900173843838\n",
      "  batch 100 loss: 0.034393909544451165\n",
      "  batch 110 loss: 0.01365290959365666\n",
      "  batch 120 loss: 0.01317817096132785\n",
      "  batch 130 loss: 0.011432019533822313\n",
      "  batch 140 loss: 0.06237269965931773\n",
      "  batch 150 loss: 0.029131969399168157\n",
      "  batch 160 loss: 0.06313598073320463\n",
      "  batch 170 loss: 0.6756720930803567\n",
      "  batch 180 loss: 0.049746694834902884\n",
      "  batch 190 loss: 0.2090197425452061\n",
      "  batch 200 loss: 0.07730877525173127\n",
      "  batch 210 loss: 0.01430670555564575\n",
      "  batch 220 loss: 0.014203615800943226\n",
      "  batch 230 loss: 0.09664892207365483\n",
      "  batch 240 loss: 0.024722549272701143\n",
      "  batch 250 loss: 0.052463571808766575\n",
      "  batch 260 loss: 0.025564363715238868\n",
      "  batch 270 loss: 0.012878650927450507\n",
      "  batch 280 loss: 0.005053199379472062\n",
      "  batch 290 loss: 0.04338773267809302\n",
      "  batch 300 loss: 0.011059177966671996\n",
      "  batch 310 loss: 0.009420574549585581\n",
      "  batch 320 loss: 0.03565710425609723\n",
      "  batch 330 loss: 0.03494827290996909\n",
      "  batch 340 loss: 0.007166415400570258\n",
      "  batch 350 loss: 0.013485842433874495\n",
      "  batch 360 loss: 0.005423920697649009\n",
      "  batch 370 loss: 0.007091838237829507\n",
      "  batch 380 loss: 0.007988878642208874\n",
      "  batch 390 loss: 0.002184084644250106\n",
      "  batch 400 loss: 0.008854522428009659\n",
      "  batch 410 loss: 0.005719967023469508\n",
      "  batch 420 loss: 0.0022904327139258384\n",
      "  batch 430 loss: 0.02855956316634547\n",
      "  batch 440 loss: 0.010375456159817987\n",
      "  batch 450 loss: 0.01159946027037222\n",
      "  batch 460 loss: 0.007381874357815832\n",
      "  batch 470 loss: 0.057505921460688114\n",
      "  batch 480 loss: 0.016754627891350536\n",
      "  batch 490 loss: 0.07560115109663457\n",
      "  batch 500 loss: 0.04867665438796394\n",
      "  batch 510 loss: 0.004626401796122082\n",
      "  batch 520 loss: 0.015673845178389455\n",
      "  batch 530 loss: 0.08214561792847234\n",
      "  batch 540 loss: 0.049383812403539194\n",
      "  batch 550 loss: 0.10731083010468864\n",
      "  batch 560 loss: 0.02277070480486145\n",
      "  batch 570 loss: 0.021552507032174616\n",
      "  batch 580 loss: 0.009741312160622328\n",
      "  batch 590 loss: 0.010523223015479744\n",
      "  batch 600 loss: 0.04146538707136642\n",
      "  batch 610 loss: 0.04591590333729982\n",
      "  batch 620 loss: 0.02263687616214156\n",
      "  batch 630 loss: 0.024784756498411296\n",
      "  batch 640 loss: 0.019644527742639185\n",
      "  batch 650 loss: 0.03377640985418111\n",
      "  batch 660 loss: 0.004855269961990416\n",
      "  batch 670 loss: 0.010728977288817988\n",
      "  batch 680 loss: 0.0008378105500014499\n",
      "  batch 690 loss: 0.02543780567939393\n",
      "  batch 700 loss: 0.03906493742251769\n",
      "  batch 710 loss: 0.04063698120880872\n",
      "  batch 720 loss: 0.007096014576382004\n",
      "  batch 730 loss: 0.10916761294356547\n",
      "  batch 740 loss: 0.026579430839046837\n",
      "  batch 750 loss: 0.007948871771804988\n",
      "  batch 760 loss: 0.16726938212523237\n",
      "  batch 770 loss: 0.11636760232504458\n",
      "  batch 780 loss: 0.07869905334664509\n",
      "  batch 790 loss: 0.0030470155412331223\n",
      "  batch 800 loss: 0.03946269592270255\n",
      "  batch 810 loss: 0.030300561850890518\n",
      "  batch 820 loss: 0.006612838411820121\n",
      "  batch 830 loss: 0.07622475748648867\n",
      "  batch 840 loss: 0.10917016192834125\n",
      "  batch 850 loss: 0.01704889073735103\n",
      "  batch 860 loss: 0.04498093705624342\n",
      "  batch 870 loss: 0.03262600860325619\n",
      "  batch 880 loss: 0.07970373504795134\n",
      "  batch 890 loss: 0.02830166348721832\n",
      "  batch 900 loss: 0.006370533246081322\n",
      "  batch 910 loss: 0.019764232076704503\n",
      "  batch 920 loss: 0.008009201334789395\n",
      "  batch 930 loss: 0.032236172643024474\n",
      "  batch 940 loss: 0.0588241585996002\n",
      "  batch 950 loss: 0.0860879159939941\n",
      "  batch 960 loss: 0.02948919076588936\n",
      "  batch 970 loss: 0.0076026979717426\n",
      "  batch 980 loss: 0.018160419195191936\n",
      "  batch 990 loss: 0.022905544249806554\n",
      "  batch 1000 loss: 0.01140001380990725\n",
      "LOSS train 0.01140001380990725 valid 0.24927707016468048\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.003925885565695353\n",
      "  batch 20 loss: 0.0013872329203877599\n",
      "  batch 30 loss: 0.01848824542830698\n",
      "  batch 40 loss: 0.005385188314539846\n",
      "  batch 50 loss: 0.008639439212856814\n",
      "  batch 60 loss: 0.014363838289864362\n",
      "  batch 70 loss: 0.003121488604665501\n",
      "  batch 80 loss: 0.021087386860745027\n",
      "  batch 90 loss: 0.008844691124977544\n",
      "  batch 100 loss: 0.02130776715930551\n",
      "  batch 110 loss: 0.029771368155343224\n",
      "  batch 120 loss: 0.001621089246509655\n",
      "  batch 130 loss: 0.012204166618175805\n",
      "  batch 140 loss: 0.0026085670862812547\n",
      "  batch 150 loss: 0.03302395525970496\n",
      "  batch 160 loss: 0.23669013804756106\n",
      "  batch 170 loss: 0.033500746660865846\n",
      "  batch 180 loss: 0.004728336256812326\n",
      "  batch 190 loss: 0.0057900563289877026\n",
      "  batch 200 loss: 0.051356863137334584\n",
      "  batch 210 loss: 0.01052611586637795\n",
      "  batch 220 loss: 0.011570389405824244\n",
      "  batch 230 loss: 0.1610539725108538\n",
      "  batch 240 loss: 0.05561672479889239\n",
      "  batch 250 loss: 0.09042308194329962\n",
      "  batch 260 loss: 0.03891755424556322\n",
      "  batch 270 loss: 0.024770753073971717\n",
      "  batch 280 loss: 0.01349348882213235\n",
      "  batch 290 loss: 0.019211174687370658\n",
      "  batch 300 loss: 0.005804450722644105\n",
      "  batch 310 loss: 0.005116618196188938\n",
      "  batch 320 loss: 0.005652097779966425\n",
      "  batch 330 loss: 0.010776631732005626\n",
      "  batch 340 loss: 0.004042824712814763\n",
      "  batch 350 loss: 0.007132358942180872\n",
      "  batch 360 loss: 0.04309699169534724\n",
      "  batch 370 loss: 0.02414926786441356\n",
      "  batch 380 loss: 0.013798644894268363\n",
      "  batch 390 loss: 0.00421035346807912\n",
      "  batch 400 loss: 0.0017189596983371302\n",
      "  batch 410 loss: 0.13597244816046442\n",
      "  batch 420 loss: 0.030096022086217998\n",
      "  batch 430 loss: 0.016655663473648018\n",
      "  batch 440 loss: 0.02698631522653159\n",
      "  batch 450 loss: 0.009494679031195118\n",
      "  batch 460 loss: 0.002786392206326127\n",
      "  batch 470 loss: 0.020531645766459404\n",
      "  batch 480 loss: 0.008924958598800003\n",
      "  batch 490 loss: 0.012271939439233392\n",
      "  batch 500 loss: 0.0023093833035090936\n",
      "  batch 510 loss: 0.002357164231943898\n",
      "  batch 520 loss: 0.0018439052568282933\n",
      "  batch 530 loss: 0.1406784270366188\n",
      "  batch 540 loss: 0.13288769692881033\n",
      "  batch 550 loss: 0.16380948499281658\n",
      "  batch 560 loss: 0.09722573430917691\n",
      "  batch 570 loss: 0.09663544325740077\n",
      "  batch 580 loss: 0.018850572500377895\n",
      "  batch 590 loss: 0.015281126194167882\n",
      "  batch 600 loss: 0.027144901218707673\n",
      "  batch 610 loss: 0.0017236038351256867\n",
      "  batch 620 loss: 0.017690379032865168\n",
      "  batch 630 loss: 0.017930425080703572\n",
      "  batch 640 loss: 0.037347365004825404\n",
      "  batch 650 loss: 0.059541880604228936\n",
      "  batch 660 loss: 0.025354778704058844\n",
      "  batch 670 loss: 0.0009203227658872492\n",
      "  batch 680 loss: 0.0016071162797743454\n",
      "  batch 690 loss: 0.0008350279691512697\n",
      "  batch 700 loss: 0.0038117042829981075\n",
      "  batch 710 loss: 0.019235742897581076\n",
      "  batch 720 loss: 0.023333768837619572\n",
      "  batch 730 loss: 0.005709547852165997\n",
      "  batch 740 loss: 0.03050785363884643\n",
      "  batch 750 loss: 0.0350420979782939\n",
      "  batch 760 loss: 0.008030224937829188\n",
      "  batch 770 loss: 0.012326810698141344\n",
      "  batch 780 loss: 0.040907660988159476\n",
      "  batch 790 loss: 0.030919045867631213\n",
      "  batch 800 loss: 0.006287278696800058\n",
      "  batch 810 loss: 0.11963200097525259\n",
      "  batch 820 loss: 0.02205694805306848\n",
      "  batch 830 loss: 0.21130919255083427\n",
      "  batch 840 loss: 0.023828811530256643\n",
      "  batch 850 loss: 0.03212927583808778\n",
      "  batch 860 loss: 0.009208698861766607\n",
      "  batch 870 loss: 0.004336974016041495\n",
      "  batch 880 loss: 0.00627958559198305\n",
      "  batch 890 loss: 0.03789832424954511\n",
      "  batch 900 loss: 0.10690842492040246\n",
      "  batch 910 loss: 0.010005599091527984\n",
      "  batch 920 loss: 0.1158402041590307\n",
      "  batch 930 loss: 0.03150322293804493\n",
      "  batch 940 loss: 0.050872765912208705\n",
      "  batch 950 loss: 0.07013128352118655\n",
      "  batch 960 loss: 0.0291101086884737\n",
      "  batch 970 loss: 0.0637670611293288\n",
      "  batch 980 loss: 0.02777276286360575\n",
      "  batch 990 loss: 0.04506569950608537\n",
      "  batch 1000 loss: 0.015161868205905194\n",
      "LOSS train 0.015161868205905194 valid 0.25857144594192505\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(loader_train, optimizer, model, loss_fn, epoch_number,report_interval) #, writer)\n",
    "\n",
    "\n",
    "    running_loss_test = 0.\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, data_test in enumerate(loader_test):\n",
    "            feature_vec_test, labels_vec_test= data_test\n",
    "            outputs_test= model(feature_vec_test)\n",
    "            loss_test = loss_fn(outputs_test, labels_vec_test)\n",
    "            running_loss_test += loss_test\n",
    "\n",
    "    avg_vloss = running_loss_test / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    #writer.add_scalars('Training vs. Validation Loss',\n",
    "    #                { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "    #                epoch_number + 1)\n",
    "    #.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772f04c",
   "metadata": {},
   "source": [
    "# Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c3119edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"model_20250828_220516_5\"\n",
    "loaded_model = model = HybridClassifier(feature_dim=feature_tensor_train.shape[1],hidden_dim=hidden_dim , num_classes=num_classes)\n",
    "loaded_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "950ffb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[942  15   7   3]\n",
      " [  8   5   1   0]\n",
      " [  5   0  10   0]\n",
      " [  4   0   0   0]]\n",
      "Precision: [0.9822732  0.25       0.55555556 0.        ]\n",
      "Sensitivity (Recall): [0.97414685 0.35714286 0.66666667 0.        ]\n",
      "Accuracy: 0.957\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():   \n",
    "        y_test_pred= model(feature_tensor_test)\n",
    "        pred_probab = nn.Softmax(dim=1)(y_test_pred)\n",
    "        pred_class = torch.argmax(pred_probab, dim=1)\n",
    "        \n",
    "\n",
    "precision = precision_score(label_tensor_test, pred_class, average=None)\n",
    "accuracy = accuracy_score(label_tensor_test, pred_class)\n",
    "recall = recall_score(label_tensor_test, pred_class, average=None)  # Sensitivity\n",
    "\n",
    "# Specificity\n",
    "conf_mat = confusion_matrix(label_tensor_test, pred_class, labels=[0,1,2,3])\n",
    "print(conf_mat)\n",
    "#specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Sensitivity (Recall): {recall}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "#print(f\"Specificity: {specificity:.3f}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf562f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
