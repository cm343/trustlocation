{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3737bad3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9f417dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388994b7",
   "metadata": {},
   "source": [
    "# Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f066ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['business_name', 'author_name', 'text', 'photo', 'rating',\n",
      "       'rating_category', 'label'],\n",
      "      dtype='object')\n",
      "business_name      object\n",
      "author_name        object\n",
      "text               object\n",
      "photo              object\n",
      "rating              int64\n",
      "rating_category    object\n",
      "label              object\n",
      "dtype: object\n",
      "                     business_name    author_name  \\\n",
      "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
      "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
      "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
      "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
      "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
      "\n",
      "                                                text  \\\n",
      "0  We went to Marmaris with my wife for a holiday...   \n",
      "1  During my holiday in Marmaris we ate here to f...   \n",
      "2  Prices are very affordable. The menu in the ph...   \n",
      "3  Turkey's cheapest artisan restaurant and its f...   \n",
      "4  I don't know what you will look for in terms o...   \n",
      "\n",
      "                                               photo  rating  \\\n",
      "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
      "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
      "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
      "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
      "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
      "\n",
      "      rating_category          label  \n",
      "0               taste  ADVERTISEMENT  \n",
      "1                menu  ADVERTISEMENT  \n",
      "2  outdoor_atmosphere           GOOD  \n",
      "3   indoor_atmosphere           GOOD  \n",
      "4                menu  ADVERTISEMENT  \n",
      "                     business_name    author_name  \\\n",
      "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
      "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
      "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
      "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
      "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
      "\n",
      "                                                text  \\\n",
      "0  We went to Marmaris with my wife for a holiday...   \n",
      "1  During my holiday in Marmaris we ate here to f...   \n",
      "2  Prices are very affordable. The menu in the ph...   \n",
      "3  Turkey's cheapest artisan restaurant and its f...   \n",
      "4  I don't know what you will look for in terms o...   \n",
      "\n",
      "                                               photo  rating  \\\n",
      "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
      "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
      "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
      "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
      "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
      "\n",
      "      rating_category          label  num_label  \n",
      "0               taste  ADVERTISEMENT          0  \n",
      "1                menu  ADVERTISEMENT          0  \n",
      "2  outdoor_atmosphere           GOOD          1  \n",
      "3   indoor_atmosphere           GOOD          1  \n",
      "4                menu  ADVERTISEMENT          0  \n",
      "0    0.892740\n",
      "1    0.071658\n",
      "2   -0.749424\n",
      "3    0.892740\n",
      "4   -0.749424\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/reviews_labeled.csv')\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "print(df.head())\n",
    "\n",
    "# Convert labels to numbers\n",
    "convert_dir = {'ADVERTISEMENT': 0, 'GOOD':1, 'IRRELEVANT':2, 'RANTS':3  }\n",
    "df['num_label'] = [convert_dir[label] for label in df['label']]\n",
    "print(df.head())\n",
    "\n",
    "# normalize tabular features\n",
    "scaler = StandardScaler()\n",
    "df[[\"rating\"]] = scaler.fit_transform(df[[\"rating\"]])\n",
    "print(df.head()[\"rating\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = df[['text', 'rating']]\n",
    "y = df['num_label']\n",
    "\n",
    "# Split into train (80%) and test (20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42, stratify=y  # stratify keeps class balance\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2aa4b3",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f71e7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4e8d708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract features\n",
    "def feature_extraction(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    vector_cls = outputs.last_hidden_state[:, 0, :]  # shape: (1, 768)\n",
    "    return vector_cls\n",
    "with torch.no_grad():\n",
    "    embeddings_train = torch.cat([feature_extraction(t) for t in x_train['text'].values])\n",
    "    embeddings_test = torch.cat([feature_extraction(t) for t in x_test['text'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "779e22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "num_samples = 1100\n",
    "text_dim = 768\n",
    "tabular_dim = 1\n",
    "hidden_dim=128\n",
    "num_classes=4\n",
    "report_interval = 10\n",
    "num_epochs = 5\n",
    "batch_size=4\n",
    "\n",
    "\n",
    "# create datatensors\n",
    "\n",
    "#train\n",
    "tabular_features_train = torch.tensor(x_train[['rating']].values, dtype=torch.float32,requires_grad=False)\n",
    "labels_train = torch.tensor(y_train.values, dtype=torch.int64,requires_grad=False)\n",
    "#test\n",
    "tabular_features_test = torch.tensor(x_test[['rating']].values, dtype=torch.float32,requires_grad=False)\n",
    "labels_test = torch.tensor(y_test.values, dtype=torch.int64,requires_grad=False)\n",
    "\n",
    "\n",
    "\n",
    "# Dataset + Dataloader\n",
    "dataset_train = TensorDataset(embeddings_train, tabular_features_train , labels_train)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(embeddings_test, tabular_features_test , labels_test)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0aa20d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "HybridClassifier(\n",
      "  (fc1): Linear(in_features=769, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "tensor([[ 0.1006, -0.0747,  0.1625, -0.2261]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2759, 0.2316, 0.2935, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([2])\n",
      "tensor(1.4630, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self, text_dim,tabular_dim , hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(text_dim + tabular_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, text_vec,tabular_vec):\n",
    "        \"\"\"\n",
    "        text_vec: [batch_size, text_dim]   (e.g., DistilBERT CLS embeddings)\n",
    "        tabular_vec: [batch_size, tabular_dim]\n",
    "        \"\"\"\n",
    "        # Concatenate text and tabular features\n",
    "        x = torch.cat([text_vec, tabular_vec], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # logits\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example setup\n",
    "model = HybridClassifier(text_dim=text_dim, tabular_dim=tabular_dim,hidden_dim=hidden_dim , num_classes=num_classes)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 5\n",
    "\n",
    "# Check forward pass\n",
    "logits = model(embeddings_test[0:1],tabular_features_test[0:1])\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "pred_class = torch.argmax(pred_probab, dim=1)\n",
    "print(logits)\n",
    "print(pred_probab)\n",
    "print(pred_class)\n",
    "\n",
    "# Check loss function\n",
    "print(loss_fn(logits,labels_test [0:1]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e1be8315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index,report_interval,):# tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, data in enumerate(loader_train):\n",
    "        # Forward pass\n",
    "        text_vec, tab_vec, labels = data\n",
    "        \n",
    "        # should not require grads\n",
    "        #print(\"text_vec.requires_grad:\", text_vec.requires_grad)\n",
    "        #print(\"tab_vec.requires_grad:\", tab_vec.requires_grad)\n",
    "        #print(\"labels.requires_grad:\", getattr(labels, \"requires_grad\", None))\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(text_vec, tab_vec)\n",
    "        \n",
    "         # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "         # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % report_interval == report_interval - 1:\n",
    "            last_loss = running_loss / report_interval # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            #tb_x = epoch_index * len(loader_train) + i + 1\n",
    "            #tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "        \n",
    "\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8c93a61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 0.8231233574450016\n",
      "  batch 20 loss: 0.9956197910010814\n",
      "  batch 30 loss: 0.9979626268148423\n",
      "  batch 40 loss: 0.8393666684627533\n",
      "  batch 50 loss: 0.449052993953228\n",
      "  batch 60 loss: 0.6304006032645703\n",
      "  batch 70 loss: 0.876692845672369\n",
      "  batch 80 loss: 0.7952616006135941\n",
      "  batch 90 loss: 0.9537592858076096\n",
      "  batch 100 loss: 0.6762700498104095\n",
      "  batch 110 loss: 0.6033520549535751\n",
      "  batch 120 loss: 0.8241837285459042\n",
      "  batch 130 loss: 0.5989247411489487\n",
      "  batch 140 loss: 0.789803110063076\n",
      "  batch 150 loss: 0.8693769961595536\n",
      "  batch 160 loss: 0.702276173233986\n",
      "  batch 170 loss: 0.8381043821573257\n",
      "  batch 180 loss: 0.5716994315385818\n",
      "  batch 190 loss: 0.5903483480215073\n",
      "  batch 200 loss: 0.6022703230381012\n",
      "  batch 210 loss: 0.7763756096363068\n",
      "  batch 220 loss: 0.827682389318943\n",
      "LOSS train 0.827682389318943 valid 0.6271569728851318\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.5029956787824631\n",
      "  batch 20 loss: 0.7142424449324608\n",
      "  batch 30 loss: 0.6112801462411881\n",
      "  batch 40 loss: 0.5334080040454865\n",
      "  batch 50 loss: 0.6331605643033982\n",
      "  batch 60 loss: 0.5780911237001419\n",
      "  batch 70 loss: 0.5712523043155671\n",
      "  batch 80 loss: 0.5053153671324253\n",
      "  batch 90 loss: 0.3474318593740463\n",
      "  batch 100 loss: 0.6756947427988053\n",
      "  batch 110 loss: 0.6336542159318924\n",
      "  batch 120 loss: 0.5396698355674744\n",
      "  batch 130 loss: 0.3755959004163742\n",
      "  batch 140 loss: 0.5330788686871528\n",
      "  batch 150 loss: 0.6804321929812431\n",
      "  batch 160 loss: 0.4654263727366924\n",
      "  batch 170 loss: 0.6385756701231002\n",
      "  batch 180 loss: 0.4564814820885658\n",
      "  batch 190 loss: 0.4357192382216454\n",
      "  batch 200 loss: 0.5008614458143711\n",
      "  batch 210 loss: 0.6073567926883697\n",
      "  batch 220 loss: 0.4105030134320259\n",
      "LOSS train 0.4105030134320259 valid 0.5550663471221924\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.30769121944904326\n",
      "  batch 20 loss: 0.3166112810373306\n",
      "  batch 30 loss: 0.6197733109816909\n",
      "  batch 40 loss: 0.6627135187387466\n",
      "  batch 50 loss: 0.3950560763478279\n",
      "  batch 60 loss: 0.4647309772670269\n",
      "  batch 70 loss: 0.5641824267804623\n",
      "  batch 80 loss: 0.6475765876471996\n",
      "  batch 90 loss: 0.5389341250061989\n",
      "  batch 100 loss: 0.4759686939418316\n",
      "  batch 110 loss: 0.4776146650314331\n",
      "  batch 120 loss: 0.42356001771986485\n",
      "  batch 130 loss: 0.4989600144326687\n",
      "  batch 140 loss: 0.5091406062245369\n",
      "  batch 150 loss: 0.4963805086910725\n",
      "  batch 160 loss: 0.5047731220722198\n",
      "  batch 170 loss: 0.4377007633447647\n",
      "  batch 180 loss: 0.31732242852449416\n",
      "  batch 190 loss: 0.49655483439564707\n",
      "  batch 200 loss: 0.3030264779925346\n",
      "  batch 210 loss: 0.5674676120281219\n",
      "  batch 220 loss: 0.6379784643650055\n",
      "LOSS train 0.6379784643650055 valid 0.5836612582206726\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.4185884013772011\n",
      "  batch 20 loss: 0.20634999126195908\n",
      "  batch 30 loss: 0.39404138214886186\n",
      "  batch 40 loss: 0.4007011063396931\n",
      "  batch 50 loss: 0.39636119455099106\n",
      "  batch 60 loss: 0.3421895921230316\n",
      "  batch 70 loss: 0.43069358021020887\n",
      "  batch 80 loss: 0.25246875658631324\n",
      "  batch 90 loss: 0.40850800052285197\n",
      "  batch 100 loss: 0.5766760408878326\n",
      "  batch 110 loss: 0.6394265234470368\n",
      "  batch 120 loss: 0.46162601858377456\n",
      "  batch 130 loss: 0.4476224139332771\n",
      "  batch 140 loss: 0.4753102585673332\n",
      "  batch 150 loss: 0.5198659330606461\n",
      "  batch 160 loss: 0.47555484920740126\n",
      "  batch 170 loss: 0.3730293855071068\n",
      "  batch 180 loss: 0.2693944938480854\n",
      "  batch 190 loss: 0.5546335086226464\n",
      "  batch 200 loss: 0.2779203303158283\n",
      "  batch 210 loss: 0.7197670191526413\n",
      "  batch 220 loss: 0.5050327211618424\n",
      "LOSS train 0.5050327211618424 valid 0.499713271856308\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.5089452184736729\n",
      "  batch 20 loss: 0.28964784294366835\n",
      "  batch 30 loss: 0.3437360398471355\n",
      "  batch 40 loss: 0.24882818795740605\n",
      "  batch 50 loss: 0.4328779488801956\n",
      "  batch 60 loss: 0.418800538033247\n",
      "  batch 70 loss: 0.37282911837100985\n",
      "  batch 80 loss: 0.27523374408483503\n",
      "  batch 90 loss: 0.47197196930646895\n",
      "  batch 100 loss: 0.5110202848911285\n",
      "  batch 110 loss: 0.3570110067725182\n",
      "  batch 120 loss: 0.46031939834356306\n",
      "  batch 130 loss: 0.6127082958817482\n",
      "  batch 140 loss: 0.39465181827545165\n",
      "  batch 150 loss: 0.2830635666847229\n",
      "  batch 160 loss: 0.19898151382803916\n",
      "  batch 170 loss: 0.6543780654668808\n",
      "  batch 180 loss: 0.4448890373110771\n",
      "  batch 190 loss: 0.27476983070373534\n",
      "  batch 200 loss: 0.433520195633173\n",
      "  batch 210 loss: 0.3789692223072052\n",
      "  batch 220 loss: 0.31044643744826317\n",
      "LOSS train 0.31044643744826317 valid 0.47436192631721497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number,report_interval) #, writer)\n",
    "\n",
    "\n",
    "    running_loss_test = 0.\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, data_test in enumerate(loader_test):\n",
    "            text_vec_test, tab_vec_test, labels_test= data_test\n",
    "            outputs_test= model(text_vec_test, tab_vec_test)\n",
    "            loss_test = loss_fn(outputs_test, labels_test)\n",
    "            running_loss_test += loss_test\n",
    "\n",
    "    avg_vloss = running_loss_test / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    #writer.add_scalars('Training vs. Validation Loss',\n",
    "    #                { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "    #                epoch_number + 1)\n",
    "    #.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96efa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
